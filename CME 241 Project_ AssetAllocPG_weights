{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Clone the rl-book repository\n","!git clone https://github.com/TikhonJelvis/rl-book.git\n","\n","# Change the working directory to the rl-book directory\n","%cd rl-book\n","\n","# Move to the branch with proper installation requirements\n","!git checkout notebook\n","!pip install -r notebooks/notebook-requirements.txt\n","!pip install -e ."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7b-7VtRk5dQ3","executionInfo":{"status":"ok","timestamp":1679039144359,"user_tz":420,"elapsed":41845,"user":{"displayName":"Rajorshi Paul","userId":"11921412492091628289"}},"outputId":"371d0e5c-5eb2-4bb8-8c2d-5507727df26d"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'rl-book'...\n","remote: Enumerating objects: 7163, done.\u001b[K\n","remote: Counting objects: 100% (890/890), done.\u001b[K\n","remote: Compressing objects: 100% (296/296), done.\u001b[K\n","remote: Total 7163 (delta 560), reused 799 (delta 531), pack-reused 6273\u001b[K\n","Receiving objects: 100% (7163/7163), 16.48 MiB | 11.93 MiB/s, done.\n","Resolving deltas: 100% (4312/4312), done.\n","/content/rl-book\n","Branch 'notebook' set up to track remote branch 'notebook' from 'origin'.\n","Switched to a new branch 'notebook'\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting more-itertools==8.4.0\n","  Downloading more_itertools-8.4.0-py3-none-any.whl (43 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.1/43.1 KB\u001b[0m \u001b[31m949.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting scipy==1.7.0\n","  Downloading scipy-1.7.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.4/28.4 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting typed-ast==1.4.1\n","  Downloading typed_ast-1.4.1-cp39-cp39-manylinux1_x86_64.whl (769 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m769.2/769.2 KB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting typing-extensions==3.7.4.3\n","  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n","Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.9/dist-packages (from scipy==1.7.0->-r notebooks/notebook-requirements.txt (line 2)) (1.22.4)\n","Installing collected packages: typing-extensions, typed-ast, scipy, more-itertools\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing_extensions 4.5.0\n","    Uninstalling typing_extensions-4.5.0:\n","      Successfully uninstalled typing_extensions-4.5.0\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.10.1\n","    Uninstalling scipy-1.10.1:\n","      Successfully uninstalled scipy-1.10.1\n","  Attempting uninstall: more-itertools\n","    Found existing installation: more-itertools 9.1.0\n","    Uninstalling more-itertools-9.1.0:\n","      Successfully uninstalled more-itertools-9.1.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","pydantic 1.10.6 requires typing-extensions>=4.2.0, but you have typing-extensions 3.7.4.3 which is incompatible.\n","bokeh 2.4.3 requires typing-extensions>=3.10.0, but you have typing-extensions 3.7.4.3 which is incompatible.\n","arviz 0.15.1 requires scipy>=1.8.0, but you have scipy 1.7.0 which is incompatible.\n","arviz 0.15.1 requires typing-extensions>=4.1.0, but you have typing-extensions 3.7.4.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed more-itertools-8.4.0 scipy-1.7.0 typed-ast-1.4.1 typing-extensions-3.7.4.3\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Obtaining file:///content/rl-book\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from rl-book==0.0.1) (3.7.1)\n","Requirement already satisfied: more-itertools in /usr/local/lib/python3.9/dist-packages (from rl-book==0.0.1) (8.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from rl-book==0.0.1) (1.22.4)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from rl-book==0.0.1) (1.4.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from rl-book==0.0.1) (1.7.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->rl-book==0.0.1) (23.0)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->rl-book==0.0.1) (8.4.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->rl-book==0.0.1) (4.39.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->rl-book==0.0.1) (0.11.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->rl-book==0.0.1) (2.8.2)\n","Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->rl-book==0.0.1) (5.12.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->rl-book==0.0.1) (1.0.7)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->rl-book==0.0.1) (3.0.9)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->rl-book==0.0.1) (1.4.4)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->rl-book==0.0.1) (2022.7.1)\n","Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->rl-book==0.0.1) (3.15.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib->rl-book==0.0.1) (1.15.0)\n","Installing collected packages: rl-book\n","  Running setup.py develop for rl-book\n","Successfully installed rl-book-0.0.1\n"]}]},{"cell_type":"code","source":["from dataclasses import dataclass\n","from typing import Sequence, Callable, Tuple, Iterator, List\n","from rl.distribution import Distribution, SampledDistribution, Gaussian\n","from rl.markov_decision_process import MarkovDecisionProcess, \\\n","    NonTerminal, State, Terminal\n","from rl.function_approx import AdamGradient, FunctionApprox, DNNSpec, \\\n","    DNNApprox\n","from rl.approximate_dynamic_programming import QValueFunctionApprox\n","from rl.approximate_dynamic_programming import ValueFunctionApprox\n","from rl.policy_gradient import reinforce_gaussian, actor_critic_gaussian, \\\n","    actor_critic_advantage_gaussian, actor_critic_td_error_gaussian\n","from rl.gen_utils.plot_funcs import plot_list_of_curves\n","import itertools\n","import numpy as np"],"metadata":{"id":"FoXNgfQC5mNk","executionInfo":{"status":"ok","timestamp":1679039144359,"user_tz":420,"elapsed":9,"user":{"displayName":"Rajorshi Paul","userId":"11921412492091628289"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":7,"metadata":{"id":"tGQfcKV_5Abl","executionInfo":{"status":"ok","timestamp":1679041560556,"user_tz":420,"elapsed":197,"user":{"displayName":"Rajorshi Paul","userId":"11921412492091628289"}}},"outputs":[],"source":["AssetAllocState = Tuple[int, float]\n","\n","\n","@dataclass(frozen=True)\n","class AssetAllocPG:\n","    risky_return_distributions: Sequence[Distribution[float]]\n","    riskless_returns: Sequence[float]\n","    utility_func: Callable[[float, int], float]\n","    policy_feature_funcs: Sequence[Callable[[AssetAllocState], float]]\n","    policy_mean_dnn_spec: DNNSpec\n","    policy_stdev: float\n","    initial_wealth_distribution: Distribution[float]\n","\n","    def time_steps(self) -> int:\n","        return len(self.risky_return_distributions)\n","\n","    def get_mdp(self) -> MarkovDecisionProcess[AssetAllocState, float]:\n","        \"\"\"\n","        State is (Wealth W_t, Time t), Action is investment in risky asset x_t\n","        Investment in riskless asset is W_t - x_t\n","        \"\"\"\n","\n","        steps: int = self.time_steps()\n","        distrs: Sequence[Distribution[float]] = self.risky_return_distributions\n","        rates: Sequence[float] = self.riskless_returns\n","        utility_f: Callable[[float, int], float] = self.utility_func\n","\n","        class AssetAllocMDP(MarkovDecisionProcess[AssetAllocState, float]):\n","\n","            def step(\n","                self,\n","                state: NonTerminal[AssetAllocState],\n","                action: float\n","                \n","            ) -> SampledDistribution[Tuple[State[AssetAllocState], float]]:\n","\n","                def sr_sampler_func(\n","                    state=state,\n","                    action=action\n","                    \n","                ) -> Tuple[State[AssetAllocState], float]:\n","                    time, wealth = state.state\n","                    next_wealth: float = action * wealth * (1 + distrs[time].sample()) \\\n","                        + wealth * (1 - action) * (1 + rates[time]) \n","                    reward: float = utility_f(next_wealth, time) \\\n","                        if time == steps - 1 else 0.\n","                    next_pair: AssetAllocState = (time + 1, next_wealth)\n","                    next_state: State[AssetAllocState] = \\\n","                        Terminal(next_pair) if time == steps - 1 \\\n","                        else NonTerminal(next_pair)\n","                    return (next_state, reward)\n","\n","                return SampledDistribution(sampler=sr_sampler_func)\n","\n","            def actions(self, state: NonTerminal[AssetAllocState]) \\\n","                    -> Sequence[float]:\n","                    return list(np.linspace(0, 1, 11))\n","\n","        return AssetAllocMDP()\n","\n","    def start_states_distribution(self) -> \\\n","            SampledDistribution[NonTerminal[AssetAllocState]]:\n","\n","        def start_states_distribution_func() -> NonTerminal[AssetAllocState]:\n","            wealth: float = self.initial_wealth_distribution.sample()\n","            return NonTerminal((0, wealth))\n","\n","        return SampledDistribution(sampler=start_states_distribution_func)\n","\n","    def policy_mean_approx(self) -> \\\n","            FunctionApprox[NonTerminal[AssetAllocState]]:\n","        adam_gradient: AdamGradient = AdamGradient(\n","            learning_rate=0.003,\n","            decay1=0.9,\n","            decay2=0.999\n","        )\n","        ffs: List[Callable[[NonTerminal[AssetAllocState]], float]] = []\n","        for f in self.policy_feature_funcs:\n","            def this_f(st: NonTerminal[AssetAllocState], f=f) -> float:\n","                return f(st.state)\n","            ffs.append(this_f)\n","        return DNNApprox.create(\n","            feature_functions=ffs,\n","            dnn_spec=self.policy_mean_dnn_spec,\n","            adam_gradient=adam_gradient\n","        )\n","\n","    def reinforce(self) -> \\\n","            Iterator[FunctionApprox[NonTerminal[AssetAllocState]]]:\n","        return reinforce_gaussian(\n","            mdp=self.get_mdp(),\n","            policy_mean_approx0=self.policy_mean_approx(),\n","            start_states_distribution=self.start_states_distribution(),\n","            policy_stdev=self.policy_stdev,\n","            gamma=1.0,\n","            episode_length_tolerance=1e-5\n","        )\n","\n","    def vf_adam_gradient(self) -> AdamGradient:\n","        return AdamGradient(\n","            learning_rate=0.003,\n","            decay1=0.9,\n","            decay2=0.999\n","        )\n","\n","    def q_value_func_approx(\n","        self,\n","        feature_functions: Sequence[Callable[\n","            [Tuple[AssetAllocState, float]], float]],\n","        dnn_spec: DNNSpec\n","    ) -> QValueFunctionApprox[AssetAllocState, float]:\n","        adam_gradient: AdamGradient = self.vf_adam_gradient()\n","        ffs: List[Callable[[Tuple[NonTerminal[\n","            AssetAllocState], float]], float]] = []\n","        for f in feature_functions:\n","            def this_f(\n","                pair: Tuple[NonTerminal[AssetAllocState], float],\n","                f=f\n","            ) -> float:\n","                return f((pair[0].state, pair[1]))\n","            ffs.append(this_f)\n","\n","        return DNNApprox.create(\n","            feature_functions=ffs,\n","            dnn_spec=dnn_spec,\n","            adam_gradient=adam_gradient\n","        )\n","\n","    def value_funcion_approx(\n","        self,\n","        feature_functions: Sequence[Callable[[AssetAllocState], float]],\n","        dnn_spec: DNNSpec\n","    ) -> ValueFunctionApprox[AssetAllocState]:\n","        adam_gradient: AdamGradient = self.vf_adam_gradient()\n","        ffs: List[Callable[[NonTerminal[AssetAllocState]], float]] = []\n","        for vf in feature_functions:\n","            def this_vf(\n","                state: NonTerminal[AssetAllocState],\n","                vf=vf\n","            ) -> float:\n","                return vf(state.state)\n","            ffs.append(this_vf)\n","\n","        return DNNApprox.create(\n","            feature_functions=ffs,\n","            dnn_spec=dnn_spec,\n","            adam_gradient=adam_gradient\n","        )\n","\n","    def actor_critic(\n","        self,\n","        feature_functions: Sequence[Callable[\n","            [Tuple[AssetAllocState, float]], float]],\n","        q_value_dnn_spec: DNNSpec\n","    ) -> Iterator[FunctionApprox[NonTerminal[AssetAllocState]]]:\n","        q_value_func_approx0: QValueFunctionApprox[AssetAllocState, float] = \\\n","            self.q_value_func_approx(feature_functions, q_value_dnn_spec)\n","\n","        return actor_critic_gaussian(\n","            mdp=self.get_mdp(),\n","            policy_mean_approx0=self.policy_mean_approx(),\n","            q_value_func_approx0=q_value_func_approx0,\n","            start_states_distribution=self.start_states_distribution(),\n","            policy_stdev=self.policy_stdev,\n","            gamma=1.0,\n","            max_episode_length=self.time_steps()\n","        )\n","\n","    def actor_critic_advantage(\n","        self,\n","        q_feature_functions: Sequence[Callable[\n","            [Tuple[AssetAllocState, float]], float]],\n","        q_dnn_spec: DNNSpec,\n","        v_feature_functions: Sequence[Callable[[AssetAllocState], float]],\n","        v_dnn_spec: DNNSpec\n","    ) -> Iterator[FunctionApprox[NonTerminal[AssetAllocState]]]:\n","        q_value_func_approx0: QValueFunctionApprox[AssetAllocState, float] = \\\n","            self.q_value_func_approx(q_feature_functions, q_dnn_spec)\n","        value_func_approx0: ValueFunctionApprox[AssetAllocState] = \\\n","            self.value_funcion_approx(v_feature_functions, v_dnn_spec)\n","        return actor_critic_advantage_gaussian(\n","            mdp=self.get_mdp(),\n","            policy_mean_approx0=self.policy_mean_approx(),\n","            q_value_func_approx0=q_value_func_approx0,\n","            value_func_approx0=value_func_approx0,\n","            start_states_distribution=self.start_states_distribution(),\n","            policy_stdev=self.policy_stdev,\n","            gamma=1.0,\n","            max_episode_length=self.time_steps()\n","        )\n","\n","    def actor_critic_td_error(\n","        self,\n","        feature_functions: Sequence[Callable[[AssetAllocState], float]],\n","        q_value_dnn_spec: DNNSpec\n","    ) -> Iterator[FunctionApprox[NonTerminal[AssetAllocState]]]:\n","        value_func_approx0: ValueFunctionApprox[AssetAllocState] = \\\n","            self.value_funcion_approx(feature_functions, q_value_dnn_spec)\n","        return actor_critic_td_error_gaussian(\n","            mdp=self.get_mdp(),\n","            policy_mean_approx0=self.policy_mean_approx(),\n","            value_func_approx0=value_func_approx0,\n","            start_states_distribution=self.start_states_distribution(),\n","            policy_stdev=self.policy_stdev,\n","            gamma=1.0,\n","            max_episode_length=self.time_steps()\n","        )  "]},{"cell_type":"code","source":["steps: int = 5\n","μ: float = 0.07\n","σ: float = 0.3\n","r: float = 0.02\n","a: float = 1.0\n","init_wealth: float = 1.0\n","init_wealth_stdev: float = 0.1\n","policy_stdev: float = 0.5\n","\n","excess: float = μ - r\n","var: float = σ * σ\n","base_alloc: float = excess / (a * var)\n","\n","risky_ret: Sequence[Gaussian] = [Gaussian(μ=μ, σ=σ) for _ in range(steps)]\n","riskless_ret: Sequence[float] = [r for _ in range(steps)]\n","\n","def time_varying_utility(w, t):\n","  # c1 = 0.25\n","  # c2 = 0.05\n","  a = 1\n","  # if t <= 25:\n","  #   c2 = 0 \n","  # else:\n","  #   c2 = 0.5\n","  # a = c1 + c2 * (t - 25)\n","  return (- np.exp(-a * w)) / a\n","\n","utility_function: Callable[[float, int], float] = time_varying_utility\n","policy_feature_funcs: Sequence[Callable[[AssetAllocState], float]] = \\\n","    [\n","        lambda w_t: (1 + r) ** w_t[1]\n","    ]\n","init_wealth_distr: Gaussian = Gaussian(μ=init_wealth, σ=init_wealth_stdev)\n","policy_mean_dnn_spec: DNNSpec = DNNSpec(\n","    neurons=[],\n","    bias=False,\n","    hidden_activation=lambda x: x,\n","    hidden_activation_deriv=lambda y: np.ones_like(y),\n","    output_activation=lambda x: x,\n","    output_activation_deriv=lambda y: np.ones_like(y)\n",")\n","\n","aad: AssetAllocPG = AssetAllocPG(\n","    risky_return_distributions=risky_ret,\n","    riskless_returns=riskless_ret,\n","    utility_func=utility_function,\n","    policy_feature_funcs=policy_feature_funcs,\n","    policy_mean_dnn_spec=policy_mean_dnn_spec,\n","    policy_stdev=policy_stdev,\n","    initial_wealth_distribution=init_wealth_distr\n",")\n","\n","reinforce_policies: Iterator[FunctionApprox[\n","    NonTerminal[AssetAllocState]]] = aad.reinforce()\n","\n","q_ffs: Sequence[Callable[[Tuple[AssetAllocState, float]], float]] = \\\n","    [\n","        lambda _: 1.,\n","        lambda wt_x: float(wt_x[0][1]),\n","        lambda wt_x: wt_x[0][0] * (1 + r) ** (- wt_x[0][1]),\n","        lambda wt_x: wt_x[1] * (1 + r) ** (- wt_x[0][1]),\n","        lambda wt_x: (wt_x[1] * (1 + r) ** (- wt_x[0][1])) ** 2,\n","    ]\n","dnn_qvf_spec: DNNSpec = DNNSpec(\n","    neurons=[],\n","    bias=False,\n","    hidden_activation=lambda x: x,\n","    hidden_activation_deriv=lambda y: np.ones_like(y),\n","    output_activation=lambda x: - np.sign(a) * np.exp(-x),\n","    output_activation_deriv=lambda y: -y\n",")\n","actor_critic_policies: Iterator[FunctionApprox[\n","    NonTerminal[AssetAllocState]]] = aad.actor_critic(\n","        feature_functions=q_ffs,\n","        q_value_dnn_spec=dnn_qvf_spec\n","    )\n","\n","v_ffs: Sequence[Callable[[AssetAllocState], float]] = \\\n","    [\n","        lambda _: 1.,\n","        lambda w_t: float(w_t[1]),\n","        lambda w_t: w_t[0] * (1 + r) ** (- w_t[1])\n","    ]\n","dnn_vf_spec: DNNSpec = DNNSpec(\n","    neurons=[],\n","    bias=False,\n","    hidden_activation=lambda x: x,\n","    hidden_activation_deriv=lambda y: np.ones_like(y),\n","    output_activation=lambda x: - np.sign(a) * np.exp(-x),\n","    output_activation_deriv=lambda y: -y\n",")\n","actor_critic_adv_policies: Iterator[FunctionApprox[\n","    NonTerminal[AssetAllocState]]] = aad.actor_critic_advantage(\n","        q_feature_functions=q_ffs,\n","        q_dnn_spec=dnn_qvf_spec,\n","        v_feature_functions=v_ffs,\n","        v_dnn_spec=dnn_vf_spec\n","    )\n","actor_critic_error_policies: Iterator[FunctionApprox[\n","    NonTerminal[AssetAllocState]]] = aad.actor_critic_td_error(\n","        feature_functions=v_ffs,\n","        q_value_dnn_spec=dnn_vf_spec\n","    )"],"metadata":{"id":"k1S0pb3A6B1t","executionInfo":{"status":"ok","timestamp":1679041582517,"user_tz":420,"elapsed":152,"user":{"displayName":"Rajorshi Paul","userId":"11921412492091628289"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["num_episodes: int = 500000\n","\n","x: Sequence[int] = range(num_episodes)\n","y0: Sequence[float] = [base_alloc * (1 + r) ** (1 - steps)] * num_episodes\n","y1: Sequence[float] = [p(NonTerminal((init_wealth, 0))) for p in\n","                        itertools.islice(reinforce_policies, num_episodes)]\n","y2: Sequence[float] = [p(NonTerminal((init_wealth, 0))) for p in\n","                        itertools.islice(\n","                            actor_critic_policies,\n","                            0,\n","                            num_episodes * steps,\n","                            steps\n","                        )]\n","y3: Sequence[float] = [p(NonTerminal((init_wealth, 0))) for p in\n","                        itertools.islice(\n","                            actor_critic_adv_policies,\n","                            0,\n","                            num_episodes * steps,\n","                            steps\n","                        )]\n","y4: Sequence[float] = [p(NonTerminal((init_wealth, 0))) for p in\n","                        itertools.islice(\n","                            actor_critic_error_policies,\n","                            0,\n","                            num_episodes * steps,\n","                            steps\n","                        )]"],"metadata":{"id":"arhrjBhQEied"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Analytical Solution\")\n","print(\"-------------------\")\n","print()\n","\n","for t in range(steps):\n","    left: int = steps - t\n","    growth: float = (1 + r) ** (left - 1)\n","    alloc: float = base_alloc / growth\n","    print(f\"Time {t:d}: Optimal Risky Allocation = {alloc:.3f}\")\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yNBIFUuOCXpj","executionInfo":{"status":"ok","timestamp":1679041591926,"user_tz":420,"elapsed":177,"user":{"displayName":"Rajorshi Paul","userId":"11921412492091628289"}},"outputId":"4104b704-6874-4a84-ad60-4c0de84ffb1f"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Analytical Solution\n","-------------------\n","\n","Time 0: Optimal Risky Allocation = 51.325\n","\n","Time 1: Optimal Risky Allocation = 52.351\n","\n","Time 2: Optimal Risky Allocation = 53.398\n","\n","Time 3: Optimal Risky Allocation = 54.466\n","\n","Time 4: Optimal Risky Allocation = 55.556\n","\n"]}]},{"cell_type":"code","source":["\n","y_vals = []\n","for y in [y0, y1, y2, y4]:\n","    y_vals.append([np.mean(y[i * plot_period:(i + 1) * plot_period])\n","                    for i in range(start, int(num_episodes / plot_period))])\n","print(x_vals)\n","print(y_vals)\n","\n","plot_list_of_curves(\n","    x_vals,\n","    y_vals,\n","    [\"k--\", \"r-x\", \"g-.\", \"b-\"],\n","    [\"True\", \"REINFORCE\", \"Actor-Critic\", \"Actor-Critic with TD Error\"],\n","    \"Iteration\",\n","    \"Action\",\n","    \"Action for Initial Wealth at Time 0\"\n",")"],"metadata":{"id":"xYouJUhJClBR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Policy Gradient Solution: Reinforce\")\n","print(\"-----------------------------------\")\n","print()\n","opt_policies: Sequence[FunctionApprox[NonTerminal[AssetAllocState]]] = \\\n","    list(itertools.islice(reinforce_policies, 10000 * steps))\n","for t in range(steps):\n","    opt_alloc: float = np.mean([p(NonTerminal((init_wealth, t)))\n","                                for p in opt_policies])\n","    print(f\"Time {t:d}: Optimal Risky Allocation = {opt_alloc:.3f}\")\n","    print()"],"metadata":{"id":"sNGXTmNqAeC-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Policy Gradient Solution: Actor Critic\")\n","print(\"--------------------------------------\")\n","print()\n","opt_policies: Sequence[FunctionApprox[NonTerminal[AssetAllocState]]] = \\\n","    list(itertools.islice(actor_critic_policies, 200000 * steps))\n","w = init_wealth\n","wealth_dist = []\n","for t in range(steps):\n","    wealth_dist.append(w)\n","    opt_alloc: float = np.mean([p(NonTerminal((init_wealth, t)))\n","                                for p in opt_policies])\n","    w = w * opt_alloc * (1 + μ) + w * (1 - opt_alloc) * (1 + r)\n","    print(f\"Time {t:d}: Optimal Risky Allocation = {opt_alloc:.3f}\")\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xvONEIhvC0Px","outputId":"435f0a48-e9f1-4650-f83e-4b583eca1a09","executionInfo":{"status":"ok","timestamp":1679043226639,"user_tz":420,"elapsed":332474,"user":{"displayName":"Rajorshi Paul","userId":"11921412492091628289"}}},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Policy Gradient Solution: Actor Critic\n","--------------------------------------\n","\n","Time 0: Optimal Risky Allocation = 4.076\n","\n","Time 1: Optimal Risky Allocation = 4.157\n","\n","Time 2: Optimal Risky Allocation = 4.240\n","\n","Time 3: Optimal Risky Allocation = 4.325\n","\n","Time 4: Optimal Risky Allocation = 4.412\n","\n"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","plt.plot(list(range(steps)), wealth_dist)\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"id":"yXNaBkQ9v2gC","executionInfo":{"status":"ok","timestamp":1679043262073,"user_tz":420,"elapsed":553,"user":{"displayName":"Rajorshi Paul","userId":"11921412492091628289"}},"outputId":"410cc0a8-a818-4388-975c-b27b6d148bd6"},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]}]}